{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-turn 1. ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import dotenv\n",
    "from operator import itemgetter\n",
    "from slack_bolt import App\n",
    "from slack_bolt.adapter.socket_mode import SocketModeHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "app = App(token=os.environ.get(\"SLACK_BOT_TOKEN\"))\n",
    "\n",
    "llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\", max_tokens=100, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" You are an assistant who can help with a variety of tasks.  \n",
    "Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
    "They can also help you with specific questions and chat about specific topics. \n",
    "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
    "If the answer is incorrect, please mention \"unsure answer\".\n",
    "\n",
    "#Chat History: {history}\n",
    "\n",
    "#Human: {human_input}\n",
    "\n",
    "#Assistant: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"human_input\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Function to create a new LLMChain with a fresh memory\n",
    "def create_chatgpt_chain():\n",
    "    memory = ConversationBufferWindowMemory(k=3)\n",
    "    return LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        verbose=True,\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "# Create a new LLMChain instance with fresh memory\n",
    "chatgpt_chain = create_chatgpt_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bolt app is running!\n",
      "{'user': 'U07D3QJHNKS', 'type': 'message', 'ts': '1721722255.682119', 'client_msg_id': 'af7f12bb-a592-4b3f-bb2b-3b68b0443469', 'text': 'ㅋㅋㅋ', 'team': 'T07CTLV9XPB', 'blocks': [{'type': 'rich_text', 'block_id': 'Q1oxi', 'elements': [{'type': 'rich_text_section', 'elements': [{'type': 'text', 'text': 'ㅋㅋㅋ'}]}]}], 'channel': 'D07C8FB6554', 'event_ts': '1721722255.682119', 'channel_type': 'im'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are an assistant who can help with a variety of tasks.  \n",
      "Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
      "They can also help you with specific questions and chat about specific topics. \n",
      "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
      "If the answer is incorrect, please mention \"unsure answer\".\n",
      "\n",
      "#Chat History: \n",
      "\n",
      "#Human: ㅋㅋㅋ\n",
      "\n",
      "#Assistant: \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'user': 'U07D3QJHNKS', 'type': 'message', 'ts': '1721722286.755109', 'client_msg_id': '5cc26c7c-6c46-4ab0-afb0-c6e53b52ebba', 'text': '오늘 날씨 좋다 그치', 'team': 'T07CTLV9XPB', 'blocks': [{'type': 'rich_text', 'block_id': 'QhhwA', 'elements': [{'type': 'rich_text_section', 'elements': [{'type': 'text', 'text': '오늘 날씨 좋다 그치'}]}]}], 'channel': 'D07C8FB6554', 'event_ts': '1721722286.755109', 'channel_type': 'im'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are an assistant who can help with a variety of tasks.  \n",
      "Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
      "They can also help you with specific questions and chat about specific topics. \n",
      "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
      "If the answer is incorrect, please mention \"unsure answer\".\n",
      "\n",
      "#Chat History: Human: ㅋㅋㅋ\n",
      "AI: 하하하! 저도 웃음이 나네요! 즐거운 하루 보내고 계시길 바랍니다. 궁금한 점이나 도움이 필요하시면 언제든지 물어보세요. 최선을 다해 도와드리겠습니다.\n",
      "\n",
      "#Human: 오늘 날씨 좋다 그치\n",
      "\n",
      "#Assistant: \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'user': 'U07D3QJHNKS', 'type': 'message', 'ts': '1721722306.750089', 'client_msg_id': '87eafe5b-9f13-4e8c-b92e-9ae138b27d72', 'text': '내 이름은 김수현이고 데이터사이언스전공이야~25살이고 신촌에살고있어', 'team': 'T07CTLV9XPB', 'blocks': [{'type': 'rich_text', 'block_id': '6uNhA', 'elements': [{'type': 'rich_text_section', 'elements': [{'type': 'text', 'text': '내 이름은 김수현이고 데이터사이언스전공이야~25살이고 신촌에살고있어'}]}]}], 'channel': 'D07C8FB6554', 'event_ts': '1721722306.750089', 'channel_type': 'im'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are an assistant who can help with a variety of tasks.  \n",
      "Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
      "They can also help you with specific questions and chat about specific topics. \n",
      "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
      "If the answer is incorrect, please mention \"unsure answer\".\n",
      "\n",
      "#Chat History: Human: ㅋㅋㅋ\n",
      "AI: 하하하! 저도 웃음이 나네요! 즐거운 하루 보내고 계시길 바랍니다. 궁금한 점이나 도움이 필요하시면 언제든지 물어보세요. 최선을 다해 도와드리겠습니다.\n",
      "Human: 오늘 날씨 좋다 그치\n",
      "AI:  네, 날씨가 정말 좋습니다! 햇볕이 따뜻하게 내리쬐고 있어서 기분도 상쾌해지네요. 야외 활동을 계획하고 계시다면 좋은 하루가 될 것 같아요.\n",
      "\n",
      "#Human: 내 이름은 김수현이고 데이터사이언스전공이야~25살이고 신촌에살고있어\n",
      "\n",
      "#Assistant: \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'user': 'U07D3QJHNKS', 'type': 'message', 'ts': '1721722688.323789', 'client_msg_id': '2e2e89c3-cca4-4d86-a4cd-f0ff8fe9527d', 'text': '내 나이는?', 'team': 'T07CTLV9XPB', 'blocks': [{'type': 'rich_text', 'block_id': 'zeVLD', 'elements': [{'type': 'rich_text_section', 'elements': [{'type': 'text', 'text': '내 나이는?'}]}]}], 'channel': 'D07C8FB6554', 'event_ts': '1721722688.323789', 'channel_type': 'im'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are an assistant who can help with a variety of tasks.  \n",
      "Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
      "They can also help you with specific questions and chat about specific topics. \n",
      "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
      "If the answer is incorrect, please mention \"unsure answer\".\n",
      "\n",
      "#Chat History: Human: ㅋㅋㅋ\n",
      "AI: 하하하! 저도 웃음이 나네요! 즐거운 하루 보내고 계시길 바랍니다. 궁금한 점이나 도움이 필요하시면 언제든지 물어보세요. 최선을 다해 도와드리겠습니다.\n",
      "Human: 오늘 날씨 좋다 그치\n",
      "AI:  네, 날씨가 정말 좋습니다! 햇볕이 따뜻하게 내리쬐고 있어서 기분도 상쾌해지네요. 야외 활동을 계획하고 계시다면 좋은 하루가 될 것 같아요.\n",
      "Human: 내 이름은 김수현이고 데이터사이언스전공이야~25살이고 신촌에살고있어\n",
      "AI:  안녕하세요, 김수현님! 만나서 반가워요. 데이터 사이언스에 관심이 있으시다니 정말 흥미롭네요. 저도 그 분야에 대해 더 알아보고 싶어졌어요. 신촌에서 생활하시는군요, 그곳은 활기찬 동네로 유명해요. 혹시 궁금한 점이나 도움이 필요하시면 언제든지 물어보세요. 최선을 다해 도와드리겠습니다.\n",
      "\n",
      "신촌에 거주하시면서 데이터 사이언스 전공을 갖고 계시다면 정말 좋은 위치를 차지하고 계신 것 같아요. 서울의 중심부에 가까워 다양한 네트워킹 기회와 인턴십, 취업 기회가 있을 거예요. 또한 신촌은 트렌디한 카페, 식당, 상점들이 많아 공부나 휴식을 취하기에 좋은 장소죠.\n",
      "\n",
      "데이터 사이언스에 대해 더 알고 싶으시다면, 데이터 분석, 머신러닝, 인공지능 등 관련 주제에 대한 자료를 추천해 드릴 수 있어요. 또한 데이터 과학 분야에서 활동하는 전문가들과 연결을 도와드릴 수도 있습니다. 궁금한 점이 있으시면 언제든 물어보세요!\n",
      "\n",
      "#Human: 내 나이는?\n",
      "\n",
      "#Assistant: \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Event handler for Slack\n",
    "@app.event(\"app_mention\")\n",
    "def handle_app_mention_events(body, say, logger):\n",
    "    message = body[\"event\"]['text']\n",
    "\n",
    "    output = chatgpt_chain.predict(human_input = message)  \n",
    "    say(output) \n",
    "\n",
    "#Message handler for Slack\n",
    "@app.message(\".*\")\n",
    "def message_handler(message, say, logger):\n",
    "    print(message)\n",
    "    \n",
    "    output = chatgpt_chain.predict(human_input = message['text'])   \n",
    "    say(output)\n",
    "\n",
    "# Start your app\n",
    "if __name__ == \"__main__\":\n",
    "    SocketModeHandler(app, os.environ[\"SLACK_APP_TOKEN\"]).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-turn 2. RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiny\\AppData\\Local\\anaconda3\\envs\\llm_pj\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import dotenv\n",
    "from operator import itemgetter\n",
    "from slack_bolt import App\n",
    "from slack_bolt.adapter.socket_mode import SocketModeHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "app = App(token=os.environ.get(\"SLACK_BOT_TOKEN\"))\n",
    "\n",
    "llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\", max_tokens=100, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiny\\AppData\\Local\\anaconda3\\envs\\llm_pj\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\" You are an assistant who can help with a variety of tasks.  \n",
    "Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
    "They can also help you with specific questions and chat about specific topics. \n",
    "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
    "If the answer is incorrect, please mention \"unsure answer\".\n",
    "\n",
    "#Chat History: {history}\n",
    "\n",
    "#Human: {human_input}\n",
    "\n",
    "#Assistant: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"human_input\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "chatgpt_chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        output_parser=StrOutputParser(),\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}                                              # 세션 기록을 저장할 딕셔너리\n",
    "\n",
    "def get_session_history(session_ids):\n",
    "    print(f\"[Conversation Session ID]: {session_ids}\")\n",
    "    if session_ids not in store:                        # 세션 ID가 store에 없는 경우                               \n",
    "        store[session_ids] = ChatMessageHistory()       # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "    return store[session_ids]                           # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(       \n",
    "    chatgpt_chain,\n",
    "    get_session_history,                               # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"human_input\",                  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"history\"                     # 기록 메시지의 키\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slack 이벤트 핸들러\n",
    "@app.event(\"app_mention\")\n",
    "def handle_app_mention_events(body, say, logger):\n",
    "    session_id = body['event']['channel']\n",
    "    message = body['event']['text']\n",
    "    history = get_session_history(session_id)\n",
    "    \n",
    "    # chain_with_history를 사용하여 응답 생성\n",
    "    response = chain_with_history.invoke(\n",
    "        {\"human_input\": message, \"history\": history},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    # 응답을 문자열로 변환하여 슬랙에 전송\n",
    "    say(response)\n",
    "\n",
    "# Slack 메시지 핸들러\n",
    "@app.message(\".*\")\n",
    "def message_handler(message, say, logger):\n",
    "    session_id = message['channel']\n",
    "    text = message['text']\n",
    "    history = get_session_history(session_id)\n",
    "\n",
    "    # chain_with_history를 사용하여 응답 생성\n",
    "    response = chain_with_history.invoke(\n",
    "        {\"human_input\": text, \"history\": history},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    # 응답을 문자열로 변환하여 슬랙에 전송\n",
    "    say(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SocketModeHandler(app, os.environ[\"SLACK_APP_TOKEN\"]).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚒️ Solving the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}  # 세션 기록을 저장할 딕셔너리\n",
    "\n",
    "def get_session_history(session_id):                                # 주어진 세션 ID에 대한 대화 기록을 가져옵니다. 세션 ID가 처음 요청된 경우에는 새로운 대화 기록 객체를 생성하여 저장합니다.\n",
    "    print(f\"[Conversation Session ID]: {session_id}\")\n",
    "    if session_id not in store:                                     # 세션 ID가 store에 없는 경우                               \n",
    "        store[session_id] = ChatMessageHistory()                    # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "    return store[session_id]                                        # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "def format_history(history):                                        # 대화 기록 객체를 포맷하여 문자열로 변환합니다.\n",
    "    messages = []\n",
    "    for msg in history.messages:\n",
    "        if isinstance(msg, HumanMessage):                           # 메시지가 HumanMessage 타입인 경우\n",
    "            messages.append(f\"Human: {msg.content}\")                # 'Human: ' 접두어와 함께 메시지 내용 추가\n",
    "        elif isinstance(msg, AIMessage):                            # 메시지가 AIMessage 타입인 경우\n",
    "            messages.append(f\"AI: {msg.content}\")                   # 'AI: ' 접두어와 함께 메시지 내용 추가\n",
    "    return \"\\n\".join(messages)                                      # 메시지 리스트를 줄 바꿈으로 결합하여 문자열 반환\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chatgpt_chain,\n",
    "    get_session_history,                                            # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"human_input\",                               # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"history\"                                  # 기록 메시지의 키\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bolt app is running!\n",
      "[Conversation Session ID]: D07C8FB6554\n",
      "[Conversation Session ID]: D07C8FB6554\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are an assistant who can help with a variety of tasks.  \n",
      "Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
      "They can also help you with specific questions and chat about specific topics. \n",
      "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
      "If the answer is incorrect, please mention \"unsure answer\".\n",
      "\n",
      "#Chat History: []\n",
      "\n",
      "#Human: 안녕! 나는 수현이라고해!\n",
      "\n",
      "#Assistant: \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "[Conversation Session ID]: D07C8FB6554\n",
      "[Conversation Session ID]: D07C8FB6554\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are an assistant who can help with a variety of tasks.  \n",
      "Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
      "They can also help you with specific questions and chat about specific topics. \n",
      "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
      "If the answer is incorrect, please mention \"unsure answer\".\n",
      "\n",
      "#Chat History: [HumanMessage(content='안녕! 나는 수현이라고해!'), AIMessage(content='안녕하세요! KPT입니다. 궁금한 점이나 도움이 필요하신 것이 있으시면 언제든지 물어보세요. 최선을 다해 도와드리겠습니다.')]\n",
      "\n",
      "#Human: 내 이름이 뭐라고 했지?\n",
      "\n",
      "#Assistant: \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Slack 이벤트 핸들러\n",
    "@app.event(\"app_mention\")\n",
    "def handle_app_mention_events(body, say, logger):\n",
    "    session_id = body['event']['channel']                           # 이벤트가 발생한 채널 ID를 세션 ID로 사용\n",
    "    message = body['event']['text']                                 # 메시지 내용을 가져옵니다.\n",
    "    history = get_session_history(session_id)                       # 해당 세션 ID의 대화 기록을 가져옵니다.\n",
    "    formatted_history = format_history(history)                     # 대화 기록을 포맷하여 문자열로 변환\n",
    "\n",
    "    # chain_with_history를 사용하여 응답 생성\n",
    "    response = chain_with_history.invoke(\n",
    "        {\"human_input\": message, \"history\": formatted_history},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    # 응답 딕셔너리에서 'text' 필드만 추출하여 슬랙에 전송합니다.\n",
    "    response_text = response.get('text', '응답을 생성하는 데 문제가 발생했습니다.')\n",
    "    say(response_text)\n",
    "\n",
    "# Slack 메시지 핸들러\n",
    "@app.message(\".*\")\n",
    "def message_handler(message, say, logger):\n",
    "    session_id = message['channel']\n",
    "    text = message['text']\n",
    "    history = get_session_history(session_id)\n",
    "    formatted_history = format_history(history)\n",
    "\n",
    "    # chain_with_history를 사용하여 응답 생성\n",
    "    response = chain_with_history.invoke(\n",
    "        {\"human_input\": text, \"history\": formatted_history},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    # 응답을 문자열로 변환하여 슬랙에 전송\n",
    "    response_text = response.get('text', '응답을 생성하는 데 문제가 발생했습니다.')\n",
    "    say(response_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SocketModeHandler(app, os.environ[\"SLACK_APP_TOKEN\"]).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✨ Summarization Chain\n",
    "\n",
    "* Multi-turn 1. ConversationBufferWindowMemory으로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_pj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
