{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌿 STEP 1-1 - Multi-turn 1. ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import dotenv\n",
    "from operator import itemgetter\n",
    "from slack_bolt import App\n",
    "from slack_bolt.adapter.socket_mode import SocketModeHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_teddynote import logging\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "logging.langsmith(\"RAG-Slack-Bot\")\n",
    "app = App(token=os.environ.get(\"SLACK_BOT_TOKEN\"))\n",
    "llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\", max_tokens=100, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" You are an assistant who can help with a variety of tasks.  \n",
    "Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
    "They can also help you with specific questions and chat about specific topics. \n",
    "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
    "If the answer is incorrect, please mention \"unsure answer\".\n",
    "\n",
    "#Chat History: {history}\n",
    "\n",
    "#Human: {human_input}\n",
    "\n",
    "#Assistant: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=template)\n",
    "\n",
    "\n",
    "# Function to create a new LLMChain with a fresh memory\n",
    "def create_chatgpt_chain():\n",
    "    memory = ConversationBufferWindowMemory(k=3)\n",
    "    return LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)\n",
    "\n",
    "\n",
    "# Create a new LLMChain instance with fresh memory\n",
    "chatgpt_chain = create_chatgpt_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event handler for Slack\n",
    "@app.event(\"app_mention\")\n",
    "def handle_app_mention_events(body, say, logger):\n",
    "    message = body[\"event\"][\"text\"]\n",
    "\n",
    "    output = chatgpt_chain.predict(human_input=message)\n",
    "    say(output)\n",
    "\n",
    "\n",
    "# Message handler for Slack\n",
    "@app.message(\".*\")\n",
    "def message_handler(message, say, logger):\n",
    "    print(message)\n",
    "\n",
    "    output = chatgpt_chain.predict(human_input=message[\"text\"])\n",
    "    say(output)\n",
    "\n",
    "\n",
    "# Start your app\n",
    "if __name__ == \"__main__\":\n",
    "    SocketModeHandler(app, os.environ[\"SLACK_APP_TOKEN\"]).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌿 STEP 2 - Multi-turn 2. RunnableWithMessageHistory\n",
    "\n",
    "`RunnableWithMessageHistory` 의 초기값에 `session_id` 키를 Default 로 삽입하는 것을 볼 수 있으며,  \n",
    "\n",
    "이 코드로 인하여 `RunnableWithMessageHistory` 는 대화 스레드 관리를 `session_id` 로 한다는 것을 간접적으로 알 수 있습니다.   \n",
    "\n",
    "즉, 대화 스레드별 관리는 `session_id` 별로 구현함을 알 수 있습니다.\n",
    "\n",
    "같은 `session_id` 를 입력하면 이전 대화 스레드의 내용을 가져오기 때문에 이어서 대화가 가능합니다!\n",
    "\n",
    "   \n",
    "\n",
    "```python\n",
    "if history_factory_config:\n",
    "    _config_specs = history_factory_config\n",
    "else:\n",
    "    # If not provided, then we'll use the default session_id field\n",
    "    _config_specs = [\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"Unique identifier for a session.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ]\n",
    "```\n",
    "\n",
    "따라서, `invoke()` 시 `config={\"configurable\": {\"session_id\": \"세션ID입력\"}}` 코드를 반드시 지정해 주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "RAG-Slack-Bot\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import dotenv\n",
    "from operator import itemgetter\n",
    "from slack_bolt import App\n",
    "from slack_bolt.adapter.socket_mode import SocketModeHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_teddynote import logging\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "logging.langsmith(\"RAG-Slack-Bot\")\n",
    "app = App(token=os.environ.get(\"SLACK_BOT_TOKEN\"))\n",
    "llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\", max_tokens=100, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\" You are an assistant who can help with a variety of tasks.  \n",
    "Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
    "They can also help you with specific questions and chat about specific topics. \n",
    "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
    "If the answer is incorrect, please mention \"unsure answer\".\n",
    "\n",
    "#Chat History: {history}\n",
    "\n",
    "#Human: {human_input}\n",
    "\n",
    "#Assistant: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=template)\n",
    "\n",
    "chatgpt_chain = LLMChain(\n",
    "    llm=llm, prompt=prompt, output_parser=StrOutputParser(), verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store = {}  # 세션 기록을 저장할 딕셔너리\n",
    "\n",
    "\n",
    "# def get_session_history(session_id):\n",
    "#     if session_id not in store:\n",
    "#         store[session_id] = ChatMessageHistory()\n",
    "#     return store[session_id]\n",
    "\n",
    "\n",
    "# def format_history(history):\n",
    "#     messages = []\n",
    "#     for msg in history.messages:\n",
    "#         if isinstance(msg, HumanMessage):\n",
    "#             messages.append(f\"Human: {msg.content}\")\n",
    "#         elif isinstance(msg, AIMessage):\n",
    "#             messages.append(f\"AI: {msg.content}\")\n",
    "#     return \"\\n\".join(messages)\n",
    "\n",
    "\n",
    "# chain_with_history = RunnableWithMessageHistory(\n",
    "#     chatgpt_chain,\n",
    "#     get_session_history,  # 세션 기록을 가져오는 함수\n",
    "#     input_messages_key=\"human_input\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "#     history_messages_key=\"history\",  # 기록 메시지의 키\n",
    "# )\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     session_id = \"default\"  # 고정된 세션 ID를 사용\n",
    "\n",
    "#     while True:\n",
    "#         human_input = input(\"입력: \")  # 사용자 입력을 콘솔에서 받기\n",
    "#         if human_input.lower() in [\"exit\", \"quit\"]:\n",
    "#             break  # 종료 조건\n",
    "\n",
    "#         history = get_session_history(session_id)\n",
    "#         formatted_history = format_history(history)\n",
    "\n",
    "#         response = chain_with_history.invoke(\n",
    "#             {\"human_input\": human_input, \"history\": formatted_history},\n",
    "#             {\"configurable\": {\"session_id\": session_id}},  # config에 session_id 추가\n",
    "#         )\n",
    "\n",
    "#         response_text = response.get(\"text\", \"응답을 생성하는 데 문제가 발생했습니다.\")\n",
    "#         print(f\"AI: {response_text}\")\n",
    "\n",
    "#         # 세션 기록 업데이트\n",
    "#         history.add_message(HumanMessage(human_input))\n",
    "#         history.add_message(AIMessage(response_text))\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slack 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}  # 세션 기록을 저장할 딕셔너리\n",
    "\n",
    "\n",
    "def get_session_history(\n",
    "    session_id,\n",
    "):  # 주어진 세션 ID에 대한 대화 기록을 가져옵니다. 세션 ID가 처음 요청된 경우에는 새로운 대화 기록 객체를 생성하여 저장합니다.\n",
    "    print(f\"[Conversation Session ID]: {session_id}\")\n",
    "    if session_id not in store:  # 세션 ID가 store에 없는 경우\n",
    "        store[session_id] = (\n",
    "            ChatMessageHistory()\n",
    "        )  # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "    return store[session_id]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "def format_history(history):  # 대화 기록 객체를 포맷하여 문자열로 변환합니다.\n",
    "    messages = []\n",
    "    for msg in history.messages:\n",
    "        if isinstance(msg, HumanMessage):  # 메시지가 HumanMessage 타입인 경우\n",
    "            messages.append(\n",
    "                f\"Human: {msg.content}\"\n",
    "            )  # 'Human: ' 접두어와 함께 메시지 내용 추가\n",
    "        elif isinstance(msg, AIMessage):  # 메시지가 AIMessage 타입인 경우\n",
    "            messages.append(\n",
    "                f\"AI: {msg.content}\"\n",
    "            )  # 'AI: ' 접두어와 함께 메시지 내용 추가\n",
    "    return \"\\n\".join(messages)  # 메시지 리스트를 줄 바꿈으로 결합하여 문자열 반환\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chatgpt_chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"human_input\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"history\",  # 기록 메시지의 키\n",
    ")\n",
    "\n",
    "\n",
    "# Slack 이벤트 핸들러\n",
    "@app.event(\"app_mention\")\n",
    "def handle_app_mention_events(body, say, logger):\n",
    "    session_id = body[\"event\"][\"channel\"]  # 이벤트가 발생한 채널 ID를 세션 ID로 사용\n",
    "    message = body[\"event\"][\"text\"]  # 메시지 내용을 가져옵니다.\n",
    "    history = get_session_history(session_id)  # 해당 세션 ID의 대화 기록을 가져옵니다.\n",
    "    formatted_history = format_history(history)  # 대화 기록을 포맷하여 문자열로 변환\n",
    "\n",
    "    # chain_with_history를 사용하여 응답 생성\n",
    "    response = chain_with_history.invoke(\n",
    "        {\"human_input\": message, \"history\": formatted_history},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "\n",
    "    # 응답 딕셔너리에서 'text' 필드만 추출하여 슬랙에 전송합니다.\n",
    "    response_text = response.get(\"text\", \"응답을 생성하는 데 문제가 발생했습니다.\")\n",
    "    say(response_text)\n",
    "\n",
    "\n",
    "# Slack 메시지 핸들러\n",
    "@app.message(\".*\")\n",
    "def message_handler(message, say, logger):\n",
    "    session_id = message[\"channel\"]\n",
    "    text = message[\"text\"]\n",
    "    history = get_session_history(session_id)\n",
    "    formatted_history = format_history(history)\n",
    "\n",
    "    # chain_with_history를 사용하여 응답 생성\n",
    "    response = chain_with_history.invoke(\n",
    "        {\"human_input\": text, \"history\": formatted_history},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "\n",
    "    # 응답을 문자열로 변환하여 슬랙에 전송\n",
    "    response_text = response.get(\"text\", \"응답을 생성하는 데 문제가 발생했습니다.\")\n",
    "    say(response_text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SocketModeHandler(app, os.environ[\"SLACK_APP_TOKEN\"]).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌿 STEP 3 - Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 👉🏻 NotionDirectoryLoader\n",
    "https://logan-vendrix.medium.com/create-your-own-notion-chatbot-with-langchain-openai-and-streamlit-fcb385f432a2\n",
    "\n",
    "#### 👉🏻 RecursiveCharacterTextSplitter\n",
    "https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.parse\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Documents length: 219\n",
      "*** Decoded content for document 1: # 대학교 이수 과목 별 제출 보고서 및 발표 자료\n",
      "\n",
      "[[과목별 보고서 & 발표 자료]-어셈블리언어](대학교 이수 과목 별 제출 보고서 및 발표 자료 9baa031ab64a474e8222d328d8b554f3/[과목별 보고서 & 발표 자료]-어셈블리언어 c05274e93077416ba857cf01009bbd83.csv)\n"
     ]
    }
   ],
   "source": [
    "def decode_url(encoded_url):\n",
    "    return urllib.parse.unquote(encoded_url)\n",
    "\n",
    "\n",
    "# 문서의 URL 인코딩된 경로를 디코딩하는 함수\n",
    "def decode_documents(docs):\n",
    "    decoded_docs = []\n",
    "    for doc in docs:\n",
    "        decoded_content = decode_url(doc.page_content)\n",
    "        decoded_doc = doc  # 기존 문서 객체를 사용\n",
    "        decoded_doc.page_content = decoded_content  # URL 디코딩된 내용으로 업데이트\n",
    "        decoded_docs.append(decoded_doc)\n",
    "    return decoded_docs\n",
    "\n",
    "\n",
    "# 파일 로딩 및 디코딩 작업\n",
    "def load_and_decode_documents(folder_path):\n",
    "    # STEP 1. 문서 로드(Load Documents)\n",
    "    loader = NotionDirectoryLoader(folder_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # STEP 2: 문서 분할(Split Documents)\n",
    "    markdown_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"#\", \"##\", \"###\", \"\\n\\n\", \"\\n\", \".\"],\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    docs = markdown_splitter.split_documents(documents)\n",
    "\n",
    "    # URL 인코딩 디코딩\n",
    "    decoded_docs = decode_documents(docs)\n",
    "\n",
    "    return decoded_docs\n",
    "\n",
    "\n",
    "# Notion content 폴더 경로\n",
    "folder_path = \"notion_content\"\n",
    "decoded_documents = load_and_decode_documents(folder_path)\n",
    "\n",
    "# 디코딩된 문서 내용 출력\n",
    "print(f\"*** Documents length: {len(decoded_documents)}\")\n",
    "print(f\"*** Decoded content for document 1: {decoded_documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded_documents[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: 임베딩(Embedding) 생성\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# STEP 4: DB 생성(Create DB) 및 저장\n",
    "vectorstore = FAISS.from_documents(documents=decoded_documents, embedding=embeddings)\n",
    "# STEP 5 : 검색기(Retriever) 생성\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'notion_content/🏫 대학교 79bb72c98b654231a8d07dc2f8f324f1/대학교 이수 과목 및 성적 094fc5a517e44a0ea05c6b81d867901d/[과목 성적] (1) dbd0203c06854a9e85e99c00194fb1e5/정보보호개론-[전공] 195677ad9ea444108cda5dc447384488.md'}, page_content='# 정보보호개론-[전공]\\n\\nTags: 1학년, 2학기\\nGrade: A+'),\n",
       " Document(metadata={'source': 'notion_content/🏫 대학교 79bb72c98b654231a8d07dc2f8f324f1/대학교 이수 과목 및 성적 094fc5a517e44a0ea05c6b81d867901d/[과목 성적] (1) dbd0203c06854a9e85e99c00194fb1e5/정보이론-[전공] 00fcccf2f4d541758925690b919730ba.md'}, page_content='# 정보이론-[전공]\\n\\nTags: 2학기, 3학년\\nGrade: A0\\n\\n##'),\n",
       " Document(metadata={'source': 'notion_content/🏫 대학교 79bb72c98b654231a8d07dc2f8f324f1/대학교 이수 과목 및 성적 094fc5a517e44a0ea05c6b81d867901d.md'}, page_content='# 대학교 이수 과목 및 성적\\n\\n[[과목 성적] (1)](대학교 이수 과목 및 성적 094fc5a517e44a0ea05c6b81d867901d/[과목 성적] (1) dbd0203c06854a9e85e99c00194fb1e5.csv)'),\n",
       " Document(metadata={'source': 'notion_content/🏫 대학교 79bb72c98b654231a8d07dc2f8f324f1/대학교 이수 과목 및 성적 094fc5a517e44a0ea05c6b81d867901d/[과목 성적] (1) dbd0203c06854a9e85e99c00194fb1e5/운영체제보안-[전공] e871cc5d9ba341e2926a8c27e8020755.md'}, page_content='# 운영체제보안-[전공]\\n\\nTags: 1학기, 3학년\\nGrade: A+')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"정보보호개론 성적은?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever 테스트 코드\n",
    "* 간단한 chain 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "취약점분석기술 과목 보고서와 관련된 파일들을 찾았습니다. 다음은 관련 문서들입니다:\n",
      "\n",
      "* 취약점분석기술_4조_최종보고서.pdf (취약점분석기술_4조/취약점분석기술_4조_최종보고서.pdf)\n",
      "* 취약점분석기술_4조_중간보고서.pdf (취약점분석기술_4조/취약점분석기술_4조_중간보고서.pdf)\n",
      "* 20194577_김수현_취약점분석기술 개인보고서.pdf (20194577_김수현_취약점분석기술 개인보고서/20194577_김수현_취약점분석기술_개인보고서.pdf)\n",
      "\n",
      "해당 파일들은 대학교 이수 과목 별 제출 보고서 및 발표 자료 폴더에 위치해 있습니다 (대학교 이수 과목 별 제출 보고서 및 발표 자료 9baa031ab64a474e8222d328d8b554f3).\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant who can help with a variety of tasks.  \n",
    "Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
    "They can also help you with specific questions and chat about specific topics. \n",
    "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
    "If the answer is incorrect, please mention \"unsure answer\".\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\", max_tokens=100, temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# question = \"대학교 교내 활동을 무엇을 했을까? 관련 문서로 된 csv나 pdf 찾아줘.\"\n",
    "question = \"취약점분석기술 과목 보고서를 작성한 적이 있었나? 있다면 관련 문서로 된 csv 형태의 파일이나 pdf 형태의 파일을 찾아주고 파일이 있는 경로도 알려줘.\"\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever 테스트 코드\n",
    "* LLMChain으로 구현\n",
    "* 코드 실행 시 다음과 같은 프롬프트는 환각 현상이 보여 수정 진행하였음\n",
    "\n",
    "==============================================================================================================================\n",
    "\n",
    "[before]  \n",
    "\n",
    "```python   \n",
    "\"\"\"You are an assistant who can help with a variety of tasks.  \n",
    "Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
    "They can also help you with specific questions and chat about specific topics. \n",
    "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
    "If the answer is incorrect, please mention \"unsure answer\".\"\"\"\n",
    "```\n",
    "\n",
    "취약점 분석 기술 과목 보고서 작성에 대한 문의에 도움을 드리겠습니다. 제공해주신 맥락과 태그를 바탕으로, 다음과 같은 관련 문서들을 찾았습니다:\n",
    "\n",
    "1. 취약점분석기술_4조_최종보고서.pdf (PDF 파일)\n",
    "2. 취약점분석기술_4조_중간보고서.pdf (PDF 파일)\n",
    "3. 20194577_김수현_취약점분석기술 개인보고서.pdf (PDF 파일)\n",
    "\n",
    "이 문서들은 모두 취약점 분석 기술 과목과 관련이 있으며, 보고서 형식으로 작성되었습니다. 해당 파일들은 제공된 URL에서 찾아보실 수 있습니다:\n",
    "\n",
    "- 취약점분석기술_4조_최종보고서.pdf: <https://drive.google.com/file/d/020d2d377f7142f389ff265e7df...>\n",
    "- 취약점분석기술_4조_중간보고서.pdf: <https://drive.google.com/file/d/bf99519803114166ac6ba...>\n",
    "- 20194577_김수현_취약점분석기술 개인보고서.pdf: <https://drive.google.com/file/d/fa8871ee...>\n",
    "\n",
    "이 파일들이 도움이 되길 바랍니다! 추가적인 질문이 있거나 더 자세한 정보가 필요하시면 언제든지 문의해 주세요.\n",
    "\n",
    "==============================================================================================================================\n",
    "\n",
    "[after]\n",
    "\n",
    "```python    \n",
    "\"\"\"Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
    "They can also help you with specific questions and chat about specific topics. \n",
    "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
    "If the answer is incorrect, please mention \"unsure answer\".\n",
    "*** Be sure to answer only using the information given, and be sure to include a separator such as '/' in the file path and specify folder information as well. ***\"\"\" \n",
    "```\n",
    "\n",
    "취약점분석기술 과목 보고서 작성을 위한 관련 문서로 된 CSV 형태의 파일이나 PDF 형태의 파일을 찾고자 한다면, 제공된 맥락에 있는 파일들을 참고하세요. 다음은 해당 파일들의 경로와 이름입니다:\n",
    "\n",
    "1. 취약점분석기술_4조_최종보고서.pdf (취약점분석기술 4조 최종 보고서)\n",
    "2. 취약점분석기술_4조_중간보고서.pdf (취약점분석기술 4조 중간 보고서)\n",
    "3. 20194577_김수현_취약점분석기술 개인보고서.pdf (20194577 김수현 취약점분석기술 개인 보고서)\n",
    "\n",
    "이 파일들은 제공된 맥락에 있는 폴더 안에 위치해 있습니다:\n",
    "\n",
    "- 취약점분석기술_4조_최종보고서.pdf: 취약점분석기술_4조/취약점분석기술_4조_최종보고서.pdf\n",
    "- 취약점분석기술_4조_중간보고서.pdf: 취약점분석기술_4조/취약점분석기술_4조_중간보고서.pdf\n",
    "- 20194577_김수현_취약점분석기술 개인보고서.pdf: 20194577_김수현/20194577_김수현_취약점분석기술_개인보고서.pdf\n",
    "\n",
    "이 파일들을 찾으려면 제공된 경로와 이름을 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
      "They can also help you with specific questions and chat about specific topics. \n",
      "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
      "If the answer is incorrect, please mention \"unsure answer\".\n",
      "*** Be sure to answer only using the information given, and be sure to include a separator such as '/' in the file path and specify folder information as well. ***.\n",
      "\n",
      "#Question: \n",
      "취약점분석기술 과목 보고서를 작성한 적이 있었나? 있다면 관련 문서로 된 csv 형태의 파일이나 pdf 형태의 파일을 찾아주고 파일이 있는 경로도 알려줘. \n",
      "\n",
      "#Context: \n",
      "[[과목별 보고서 & 발표 자료]-취약점분석기술 (Team Project)](대학교 이수 과목 별 제출 보고서 및 발표 자료 9baa031ab64a474e8222d328d8b554f3/[과목별 보고서 & 발표 자료]-취약점분석기술 b837089ae9044ffcbb7911edef07eaa7.csv)\n",
      "# 취약점분석기술_4조_최종보고서\n",
      "\n",
      "Tags: Report\n",
      "\n",
      "[취약점분석기술_4조_최종보고서.pdf](취약점분석기술_4조_최종보고서 020d2d377f7142f389ff265e7dff340c/취약점분석기술_4조_최종보고서.pdf)\n",
      "# 취약점분석기술_4조_중간보고서\n",
      "\n",
      "Tags: Report\n",
      "\n",
      "[취약점분석기술_4조_중간보고서.pdf](취약점분석기술_4조_중간보고서 bf99519803114166ac6bab758ad4b730/취약점분석기술_4조_중간보고서.pdf)\n",
      "# 20194577_김수현_취약점분석기술 개인보고서\n",
      "\n",
      "Tags: Report\n",
      "\n",
      "[20194577_김수현_취약점분석기술 개인보고서.pdf](20194577_김수현_취약점분석기술 개인보고서 fa8871eef3fd4c5ab136743ee04e03a9/20194577_김수현_취약점분석기술_개인보고서.pdf) \n",
      "\n",
      "#Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "취약점분석기술 과목 보고서 작성을 위한 관련 문서로 된 CSV 형태의 파일이나 PDF 형태의 파일을 찾고자 한다면, 제공된 맥락에 있는 파일들을 참고하세요. 다음은 해당 파일들의 경로와 이름입니다:\n",
      "\n",
      "1. 취약점분석기술_4조_최종보고서.pdf (취약점분석기술 4조 최종 보고서)\n",
      "2. 취약점분석기술_4조_중간보고서.pdf (취약점분석기술 4조 중간 보고서)\n",
      "3. 20194577_김수현_취약점분석기술 개인보고서.pdf (20194577 김수현 취약점분석기술 개인 보고서)\n",
      "\n",
      "이 파일들은 제공된 맥락에 있는 폴더 안에 위치해 있습니다:\n",
      "\n",
      "- 취약점분석기술_4조_최종보고서.pdf: 취약점분석기술_4조/취약점분석기술_4조_최종보고서.pdf\n",
      "- 취약점분석기술_4조_중간보고서.pdf: 취약점분석기술_4조/취약점분석기술_4조_중간보고서.pdf\n",
      "- 20194577_김수현_취약점분석기술 개인보고서.pdf: 20194577_김수현/20194577_김수현_취약점분석기술_개인보고서.pdf\n",
      "\n",
      "이 파일들을 찾으려면 제공된 경로와 이름을 참고하세요.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# retriever 설정\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 프롬프트 템플릿 설정\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Your name is KPT. Please be sure to answer in Korean. Can provide valuable insight and information on a wide range of topics. \n",
    "They can also help you with specific questions and chat about specific topics. \n",
    "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
    "If the answer is incorrect, please mention \"unsure answer\".\n",
    "*** Be sure to answer only using the information given, and be sure to include a separator such as '/' in the file path and specify folder information as well. ***.\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# LLM 설정\n",
    "llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\", max_tokens=100, temperature=0)\n",
    "\n",
    "# LLMChain 설정\n",
    "chain = LLMChain(prompt=prompt, llm=llm, output_parser=StrOutputParser(), verbose=True)\n",
    "\n",
    "\n",
    "def retrieve_documents(human_input, num_results=5):\n",
    "    # retriever를 이용하여 문서 검색\n",
    "    results = retriever.get_relevant_documents(human_input, k=num_results)\n",
    "    # 검색된 결과를 하나의 문자열로 병합\n",
    "    return \"\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "\n",
    "# 질문과 문서 검색 결과를 체인에 넣어 실행\n",
    "def main():\n",
    "    question = \"취약점분석기술 과목 보고서를 작성한 적이 있었나? 있다면 관련 문서로 된 csv 형태의 파일이나 pdf 형태의 파일을 찾아주고 파일이 있는 경로도 알려줘.\"\n",
    "\n",
    "    context = retrieve_documents(question, num_results=5)\n",
    "    inputs = {\"question\": question, \"context\": context}\n",
    "\n",
    "    response = chain.invoke(inputs)\n",
    "    response_text = response.get(\"text\", \"응답을 생성하는 데 문제가 발생했습니다.\")\n",
    "    print(response_text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever & Model 연동 - Kernel 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\", max_tokens=100, temperature=0)\n",
    "\n",
    "template = \"\"\"Your name is KPT. Please be sure to answer in Korean. \n",
    "Can provide valuable insight and information on a wide range of topics. \n",
    "They can also help you with specific questions and chat about specific topics. \n",
    "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
    "If the answer is incorrect, please mention \"unsure answer\".\n",
    "*** Please mention and answer only the content related to Human's question! If the purpose is to greet you, just say hello, and if you are asking for information, ask them to provide the information. ***\n",
    "*** Be sure to answer only using the information given, and be sure to include a separator such as '/' in the file path and specify folder information as well. ***\n",
    "\n",
    "#Chat History: {history}\n",
    "\n",
    "#Context: {context}\n",
    "\n",
    "#Human: {human_input}\n",
    "\n",
    "#Assistant: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"human_input\", \"context\"], template=template\n",
    ")\n",
    "\n",
    "chatgpt_chain = LLMChain(\n",
    "    llm=llm, prompt=prompt, output_parser=StrOutputParser(), verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour name is KPT. Please be sure to answer in Korean. \n",
      "Can provide valuable insight and information on a wide range of topics. \n",
      "They can also help you with specific questions and chat about specific topics. \n",
      "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
      "If the answer is incorrect, please mention \"unsure answer\".\n",
      "*** Please mention and answer only the content related to Human's question! If the purpose is to greet you, just say hello, and if you are asking for information, ask them to provide the information. ***\n",
      "*** Be sure to answer only using the information given, and be sure to include a separator such as '/' in the file path and specify folder information as well. ***\n",
      "\n",
      "#Chat History: []\n",
      "\n",
      "#Context: # 제목 없음\n",
      "# 어셈블리언어-[전공]\n",
      "\n",
      "Tags: 1학기, 2학년\n",
      "Grade: A+\n",
      "# 전공영어-[전공]\n",
      "\n",
      "Tags: 1학기, 3학년\n",
      "Grade: A0\n",
      "# 환경과에너지-[교양]\n",
      "\n",
      "Tags: 1학기, 3학년\n",
      "Grade: A0\n",
      "\n",
      "#Human: 안녕?\n",
      "\n",
      "#Assistant: \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: 안녕하세요! 궁금한 점이나 도움이 필요하신 것이 있으시면 언제든지 물어보세요. 주어진 정보 안에서 최선을 다해 도와드리겠습니다. 저는 AI 어시스턴트로서 정확하고 유용한 답변을 드리기 위해 노력하고 있습니다. 만약 질문에 대한 확실한 답을 모르거나 확신이 서지 않는다면, '확실한 답변이 없습니다'라고 말씀드릴게요. 또한, 여러분의 질문이 관련성이 있거나 말이 되는지를 확인하여 안전하고 존중하는 대화를 유지하도록 하겠습니다. 궁금한 점이 있으시면 언제든지 물어보세요!\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour name is KPT. Please be sure to answer in Korean. \n",
      "Can provide valuable insight and information on a wide range of topics. \n",
      "They can also help you with specific questions and chat about specific topics. \n",
      "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
      "If the answer is incorrect, please mention \"unsure answer\".\n",
      "*** Please mention and answer only the content related to Human's question! If the purpose is to greet you, just say hello, and if you are asking for information, ask them to provide the information. ***\n",
      "*** Be sure to answer only using the information given, and be sure to include a separator such as '/' in the file path and specify folder information as well. ***\n",
      "\n",
      "#Chat History: [HumanMessage(content='안녕?'), AIMessage(content=\"안녕하세요! 궁금한 점이나 도움이 필요하신 것이 있으시면 언제든지 물어보세요. 주어진 정보 안에서 최선을 다해 도와드리겠습니다. 저는 AI 어시스턴트로서 정확하고 유용한 답변을 드리기 위해 노력하고 있습니다. 만약 질문에 대한 확실한 답을 모르거나 확신이 서지 않는다면, '확실한 답변이 없습니다'라고 말씀드릴게요. 또한, 여러분의 질문이 관련성이 있거나 말이 되는지를 확인하여 안전하고 존중하는 대화를 유지하도록 하겠습니다. 궁금한 점이 있으시면 언제든지 물어보세요!\"), HumanMessage(content='안녕?'), AIMessage(content=\"안녕하세요! 궁금한 점이나 도움이 필요하신 것이 있으시면 언제든지 물어보세요. 주어진 정보 안에서 최선을 다해 도와드리겠습니다. 저는 AI 어시스턴트로서 정확하고 유용한 답변을 드리기 위해 노력하고 있습니다. 만약 질문에 대한 확실한 답을 모르거나 확신이 서지 않는다면, '확실한 답변이 없습니다'라고 말씀드릴게요. 또한, 여러분의 질문이 관련성이 있거나 말이 되는지를 확인하여 안전하고 존중하는 대화를 유지하도록 하겠습니다. 궁금한 점이 있으시면 언제든지 물어보세요!\")]\n",
      "\n",
      "#Context: # 대학교 이수 과목 및 성적\n",
      "\n",
      "[[과목 성적] (1)](대학교 이수 과목 및 성적 094fc5a517e44a0ea05c6b81d867901d/[과목 성적] (1) dbd0203c06854a9e85e99c00194fb1e5.csv)\n",
      "# 정보보호개론-[전공]\n",
      "\n",
      "Tags: 1학년, 2학기\n",
      "Grade: A+\n",
      "# 정보이론-[전공]\n",
      "\n",
      "Tags: 2학기, 3학년\n",
      "Grade: A0\n",
      "\n",
      "##\n",
      "저는 매우 다정하고 외향적입니다. 저는 다른 나라의 사람들과 잘 일하고 고객들을 돕는 것을 즐깁니다.\n",
      "\n",
      "**(7) Technical Skills (기술력)**\n",
      "\n",
      "I can use Python and Javascript and I have a lot of experience with the SiO2 cyber security application.\n",
      "\n",
      "저는 파이썬과 자바스크립트를 사용할 수 있으며 SiO2 사이버 보안 어플리케이션에 많은 경험이 있습니다.\n",
      "\n",
      "#Human: 혹시 내 성적에 관한 정보가 있다면 과목과 성적들을 알려줄래?\n",
      "\n",
      "#Assistant: \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: 안녕하세요! 궁금한 점이나 도움이 필요하시면 언제든지 물어보세요. 주어진 정보 안에서 최선을 다해 도와드리겠습니다. 저는 AI 어시스턴트로 정확하고 유용한 답변을 드리기 위해 노력하고 있습니다. 만약 질문에 대한 확실한 답을 모르거나 확신할 수 없다면 '확실한 답변이 없습니다'라고 말씀드릴게요. 또한, 여러분의 질문과 관련하여 안전하고 존중하는 대화를 유지하도록 하겠습니다.\n",
      "\n",
      "성적에 관한 정보를 제공해 주셔서 감사합니다! 제공해주신 내용을 바탕으로 다음과 같은 과목들과 성적을 확인했습니다:\n",
      "\n",
      "* 정보보호개론 (A+) - 1학년 2학기\n",
      "* 정보이론 (A0) - 3학년 2학기\n",
      "\n",
      "도움이 되셨길 바라며, 추가적인 질문이 있으시면 언제든지 물어보세요!\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour name is KPT. Please be sure to answer in Korean. \n",
      "Can provide valuable insight and information on a wide range of topics. \n",
      "They can also help you with specific questions and chat about specific topics. \n",
      "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
      "If the answer is incorrect, please mention \"unsure answer\".\n",
      "*** Please mention and answer only the content related to Human's question! If the purpose is to greet you, just say hello, and if you are asking for information, ask them to provide the information. ***\n",
      "*** Be sure to answer only using the information given, and be sure to include a separator such as '/' in the file path and specify folder information as well. ***\n",
      "\n",
      "#Chat History: [HumanMessage(content='안녕?'), AIMessage(content=\"안녕하세요! 궁금한 점이나 도움이 필요하신 것이 있으시면 언제든지 물어보세요. 주어진 정보 안에서 최선을 다해 도와드리겠습니다. 저는 AI 어시스턴트로서 정확하고 유용한 답변을 드리기 위해 노력하고 있습니다. 만약 질문에 대한 확실한 답을 모르거나 확신이 서지 않는다면, '확실한 답변이 없습니다'라고 말씀드릴게요. 또한, 여러분의 질문이 관련성이 있거나 말이 되는지를 확인하여 안전하고 존중하는 대화를 유지하도록 하겠습니다. 궁금한 점이 있으시면 언제든지 물어보세요!\"), HumanMessage(content='안녕?'), AIMessage(content=\"안녕하세요! 궁금한 점이나 도움이 필요하신 것이 있으시면 언제든지 물어보세요. 주어진 정보 안에서 최선을 다해 도와드리겠습니다. 저는 AI 어시스턴트로서 정확하고 유용한 답변을 드리기 위해 노력하고 있습니다. 만약 질문에 대한 확실한 답을 모르거나 확신이 서지 않는다면, '확실한 답변이 없습니다'라고 말씀드릴게요. 또한, 여러분의 질문이 관련성이 있거나 말이 되는지를 확인하여 안전하고 존중하는 대화를 유지하도록 하겠습니다. 궁금한 점이 있으시면 언제든지 물어보세요!\"), HumanMessage(content='혹시 내 성적에 관한 정보가 있다면 과목과 성적들을 알려줄래?'), AIMessage(content=\"안녕하세요! 궁금한 점이나 도움이 필요하시면 언제든지 물어보세요. 주어진 정보 안에서 최선을 다해 도와드리겠습니다. 저는 AI 어시스턴트로 정확하고 유용한 답변을 드리기 위해 노력하고 있습니다. 만약 질문에 대한 확실한 답을 모르거나 확신할 수 없다면 '확실한 답변이 없습니다'라고 말씀드릴게요. 또한, 여러분의 질문과 관련하여 안전하고 존중하는 대화를 유지하도록 하겠습니다.\\n\\n성적에 관한 정보를 제공해 주셔서 감사합니다! 제공해주신 내용을 바탕으로 다음과 같은 과목들과 성적을 확인했습니다:\\n\\n* 정보보호개론 (A+) - 1학년 2학기\\n* 정보이론 (A0) - 3학년 2학기\\n\\n도움이 되셨길 바라며, 추가적인 질문이 있으시면 언제든지 물어보세요!\"), HumanMessage(content='혹시 내 성적에 관한 정보가 있다면 과목과 성적들을 알려줄래?'), AIMessage(content=\"안녕하세요! 궁금한 점이나 도움이 필요하시면 언제든지 물어보세요. 주어진 정보 안에서 최선을 다해 도와드리겠습니다. 저는 AI 어시스턴트로 정확하고 유용한 답변을 드리기 위해 노력하고 있습니다. 만약 질문에 대한 확실한 답을 모르거나 확신할 수 없다면 '확실한 답변이 없습니다'라고 말씀드릴게요. 또한, 여러분의 질문과 관련하여 안전하고 존중하는 대화를 유지하도록 하겠습니다.\\n\\n성적에 관한 정보를 제공해 주셔서 감사합니다! 제공해주신 내용을 바탕으로 다음과 같은 과목들과 성적을 확인했습니다:\\n\\n* 정보보호개론 (A+) - 1학년 2학기\\n* 정보이론 (A0) - 3학년 2학기\\n\\n도움이 되셨길 바라며, 추가적인 질문이 있으시면 언제든지 물어보세요!\")]\n",
      "\n",
      "#Context: # 정보보호개론-[전공]\n",
      "\n",
      "Tags: 1학년, 2학기\n",
      "Grade: A+\n",
      "# 정수론-[전공]\n",
      "\n",
      "Tags: 1학기, 2학년\n",
      "Grade: A+\n",
      "# 현대사회와생명과학-[교양]\n",
      "\n",
      "Tags: 2학기, 2학년\n",
      "Grade: A+\n",
      "# 어셈블리언어-[전공]\n",
      "\n",
      "Tags: 1학기, 2학년\n",
      "Grade: A+\n",
      "\n",
      "#Human: 1학년 2학기 성적들을 알려줘\n",
      "\n",
      "#Assistant: \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: 안녕하세요! 궁금한 점이나 도움이 필요하시면 언제든지 물어보세요. 주어진 정보 안에서 최선을 다해 도와드리겠습니다. 저는 AI 어시스턴트로 정확하고 유용한 답변을 드리기 위해 노력하고 있습니다. 만약 질문에 대한 확실한 답을 모르거나 확신할 수 없다면 '확실한 답변이 없습니다'라고 말씀드릴게요. 또한, 여러분의 질문과 관련하여 안전하고 존중하는 대화를 유지하도록 하겠습니다.\n",
      "\n",
      "1학년 2학기 성적에 대해 알려드리겠습니다! 제공해 주신 내용을 바탕으로 다음과 같은 과목들과 성적을 확인했습니다:\n",
      "\n",
      "* 정보보호개론 (A+) - 1학년 2학기\n",
      "\n",
      "도움이 되셨길 바라며, 추가적인 질문이 있으시면 언제든지 물어보세요!\n"
     ]
    }
   ],
   "source": [
    "# 세션 기록을 저장할 딕셔너리\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "def format_history(history):\n",
    "    messages = []\n",
    "    for msg in history.messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            messages.append(f\"Human: {msg.content}\")\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            messages.append(f\"AI: {msg.content}\")\n",
    "    return \"\\n\".join(messages)\n",
    "\n",
    "\n",
    "def retrieve_documents(human_input):\n",
    "    # def retrieve_documents(human_input, num_results=5):\n",
    "    # retriever를 이용하여 문서 검색\n",
    "    results = retriever.get_relevant_documents(human_input)\n",
    "    # results = retriever.get_relevant_documents(human_input, k=num_results)\n",
    "    # 검색된 결과를 하나의 문자열로 병합\n",
    "    return \"\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chatgpt_chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"human_input\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"history\",  # 기록 메시지의 키\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    session_id = \"default\"  # 고정된 세션 ID를 사용\n",
    "\n",
    "    while True:\n",
    "        human_input = input(\"입력: \")  # 사용자 입력을 콘솔에서 받기\n",
    "        if human_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break  # 종료 조건\n",
    "\n",
    "        history = get_session_history(session_id)\n",
    "        formatted_history = format_history(history)\n",
    "\n",
    "        # 사용자 입력을 기반으로 문서 검색\n",
    "        context = retrieve_documents(human_input)\n",
    "\n",
    "        response = chain_with_history.invoke(\n",
    "            {\n",
    "                \"human_input\": human_input,\n",
    "                \"history\": formatted_history,\n",
    "                \"context\": context,\n",
    "            },\n",
    "            {\"configurable\": {\"session_id\": session_id}},  # config에 session_id 추가\n",
    "        )\n",
    "\n",
    "        response_text = response.get(\"text\", \"응답을 생성하는 데 문제가 발생했습니다.\")\n",
    "        print(f\"AI: {response_text}\")\n",
    "\n",
    "        # 세션 기록 업데이트\n",
    "        history.add_message(HumanMessage(human_input))\n",
    "        history.add_message(AIMessage(response_text))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌿 FIANL STEP  - Slack과 Retriever- Multiturn Model 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.parse\n",
    "from slack_bolt import App\n",
    "from slack_bolt.adapter.socket_mode import SocketModeHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "app = App(token=os.environ[\"SLACK_BOT_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 설정\n",
    "template = \"\"\"Your name is KPT. Please be sure to answer in Korean. \n",
    "Can provide valuable insight and information on a wide range of topics. \n",
    "They can also help you with specific questions and chat about specific topics. \n",
    "Please refer to the \"Chat History\" and keep your answer short and concise with 2-3 sentences.\n",
    "If the answer is incorrect, please mention \"unsure answer\".\n",
    "*** Please mention and answer only the content related to Human's question! If the purpose is to greet you, just say hello, and if you are asking for information, ask them to provide the information. ***\n",
    "*** Be sure to answer only using the information given, and be sure to include a separator such as '/' in the file path and specify folder information as well. ***\n",
    "\n",
    "#Chat History: {history}\n",
    "\n",
    "#Context: {context}\n",
    "\n",
    "#Human: {human_input}\n",
    "\n",
    "#Assistant: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"human_input\", \"context\"], template=template\n",
    ")\n",
    "\n",
    "# LLM 설정\n",
    "llm = ChatOllama(model=\"EEVE-Korean-10.8B:latest\", max_tokens=100, temperature=0)\n",
    "\n",
    "# LLMChain 설정\n",
    "chatgpt_chain = LLMChain(\n",
    "    llm=llm, prompt=prompt, output_parser=StrOutputParser(), verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션 기록을 저장할 딕셔너리\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "def format_history(history):\n",
    "    messages = []\n",
    "    for msg in history.messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            messages.append(f\"Human: {msg.content}\")\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            messages.append(f\"AI: {msg.content}\")\n",
    "    return \"\\n\".join(messages)\n",
    "\n",
    "\n",
    "def retrieve_documents(human_input, num_results=5):\n",
    "    # retriever를 이용하여 문서 검색\n",
    "    results = retriever.get_relevant_documents(human_input, k=num_results)\n",
    "    # 검색된 결과를 하나의 문자열로 병합\n",
    "    return \"\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "\n",
    "# chain_with_history 설정\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chatgpt_chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"human_input\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"history\",  # 기록 메시지의 키\n",
    ")\n",
    "\n",
    "\n",
    "# Slack 이벤트 핸들러\n",
    "@app.event(\"app_mention\")\n",
    "def handle_app_mention_events(body, say, logger):\n",
    "    session_id = body[\"event\"][\"channel\"]  # 이벤트가 발생한 채널 ID를 세션 ID로 사용\n",
    "    message = body[\"event\"][\"text\"]  # 메시지 내용을 가져옵니다.\n",
    "    history = get_session_history(session_id)  # 해당 세션 ID의 대화 기록을 가져옵니다.\n",
    "    formatted_history = format_history(history)  # 대화 기록을 포맷하여 문자열로 변환\n",
    "\n",
    "    # 컨텍스트를 검색하여 문서 병합\n",
    "    context = retrieve_documents(message)\n",
    "\n",
    "    # chain_with_history를 사용하여 응답 생성\n",
    "    response = chain_with_history.invoke(\n",
    "        {\"human_input\": message, \"history\": formatted_history, \"context\": context},\n",
    "        {\"configurable\": {\"session_id\": session_id}},  # config에 session_id 추가\n",
    "    )\n",
    "\n",
    "    # 응답 딕셔너리에서 'text' 필드만 추출하여 슬랙에 전송합니다.\n",
    "    response_text = response.get(\"text\", \"응답을 생성하는 데 문제가 발생했습니다.\")\n",
    "    say(response_text)\n",
    "\n",
    "    # 세션 기록 업데이트\n",
    "    history.add_message(HumanMessage(message))\n",
    "    history.add_message(AIMessage(response_text))\n",
    "\n",
    "\n",
    "# Slack 메시지 핸들러\n",
    "@app.message(\".*\")\n",
    "def message_handler(message, say, logger):\n",
    "    session_id = message[\"channel\"]\n",
    "    text = message[\"text\"]\n",
    "    history = get_session_history(session_id)\n",
    "    formatted_history = format_history(history)\n",
    "\n",
    "    # 컨텍스트를 검색하여 문서 병합\n",
    "    context = retrieve_documents(text)\n",
    "\n",
    "    # chain_with_history를 사용하여 응답 생성\n",
    "    response = chain_with_history.invoke(\n",
    "        {\"human_input\": text, \"history\": formatted_history, \"context\": context},\n",
    "        {\"configurable\": {\"session_id\": session_id}},  # config에 session_id 추가\n",
    "    )\n",
    "\n",
    "    # 응답을 문자열로 변환하여 슬랙에 전송\n",
    "    response_text = response.get(\"text\", \"응답을 생성하는 데 문제가 발생했습니다.\")\n",
    "    say(response_text)\n",
    "\n",
    "    # 세션 기록 업데이트\n",
    "    history.add_message(HumanMessage(text))\n",
    "    history.add_message(AIMessage(response_text))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SocketModeHandler(app, os.environ[\"SLACK_APP_TOKEN\"]).start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_pj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
