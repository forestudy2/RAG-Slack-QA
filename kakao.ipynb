{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/aifactory_ds2/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e80edea77fa4997abdb290d66134149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import dotenv \n",
    "\n",
    "dotenv.load_dotenv()\n",
    "login(token=os.environ.get(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "model_id = \"allganize/Llama-3-Alpha-Ko-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# prompt = PromptTemplate.from_template(\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "#                                       너는 user 질문에 정보의 내용대로 답하는 assistant야. user가 질문하는 내용을 정보를 이용해서 정확하고 최대한 자세하게 답해. 질문 내용이 정보에 없으면 '이 질문은 답변할 수 없습니다'라고 답해. 한국어(Korean)로 대답해.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "#                                       안녕하세요!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "#                                       안녕! 만나서 반가워요!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "#                                       ### 질문: {question}\n",
    "#                                       ### 답변<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "#                                       \"\"\")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "                                      너는 user 질문에 정보의 내용대로 답하는 assistant야. user가 질문하는 내용을 정보를 이용해서 정확하고 최대한 자세하게 답해. 질문 내용이 정보에 없으면 '이 질문은 답변할 수 없습니다'라고 답해. 한국어(Korean)로 대답해.\n",
    "                                      ### 질문: {question}\n",
    "                                      ### 답변<|eot_id|>\n",
    "                                      <|start_header_id|>assistant<|end_header_id|>\n",
    "                                      \"\"\")\n",
    "\n",
    "chain  = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[대화 세션ID]: history\n",
      "테디노트라는 이름은 특별하고 멋진 이름이에요. 테디노트라는 이름을 가진 분은 누구나 특별하고 독특한 존재임을 증명해줘요. 이름에 담긴 의미나 특별한 기억이 있으신가요?assistant\n",
      "이 질문은 답변할 수 없습니다assistant\n",
      "이 질문은 답변할 수 없습니다assistant\n",
      "이 질문은 답변할 수 없습니다assistant\n",
      "이 질문은 답변할 수 없습니다assistant\n",
      "이 질문은 답변할 수 없습니다assistant\n",
      "이 질문은 답변할 수 없습니다assistant\n",
      "이 질문은 답변할 수 없습니다assistant\n",
      "이 질문은 답변할 수 없습니다\n",
      "[대화 세션ID]: history\n",
      "이 질문은 답변할 수 없습니다\n"
     ]
    }
   ],
   "source": [
    "# 세션 기록을 저장할 딕셔너리\n",
    "store = {}\n",
    "\n",
    "\n",
    "# 세션 ID를 기반으로 세션 기록을 가져오는 함수\n",
    "def get_session_history(session_ids : str):\n",
    "    print(f\"[대화 세션ID]: {session_ids}\")\n",
    "    if session_ids not in store:  # 세션 ID가 store에 없는 경우\n",
    "        # 새로운 ChatMessageHistory 객체를 생성하여 store에 저장\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # 해당 세션 ID에 대한 세션 기록 반환\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # 세션 기록을 가져오는 함수\n",
    "    input_messages_key=\"question\",  # 사용자의 질문이 템플릿 변수에 들어갈 key\n",
    "    history_messages_key=\"history\",  # 기록 메시지의 키\n",
    "    # output_messages_key=\"output_message\"\n",
    ")\n",
    "output = chain_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"내 이름은 테디노트라구해\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"history\"}},\n",
    ")\n",
    "print(output.split(\"### 답변<|eot_id|><|start_header_id|>assistant<|end_header_id|>\")[1].replace(\"\\n                                       \",\"\"))\n",
    "output = chain_with_history.invoke(\n",
    "    # 질문 입력\n",
    "    {\"question\": \"내 이름이 뭐라고?\"},\n",
    "    # 세션 ID 기준으로 대화를 기록합니다.\n",
    "    config={\"configurable\": {\"session_id\": \"history\"}},\n",
    ")\n",
    "print(output.split(\"### 답변<|eot_id|><|start_header_id|>assistant<|end_header_id|>\")[1].replace(\"\\n                                       \",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
